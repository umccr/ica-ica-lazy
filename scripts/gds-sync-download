#!/usr/bin/env bash

: '
Obtain the aws access credentials for a gds folder then run the aws sync command to the destination directory
'

# Globals
SECONDS_PER_WEEK="604800"

# Help function
print_help(){
  echo "
        Usage: gds-sync-download (--gds-path gds://volume-name/path-to-folder/)
                                 (--download-path downloads/)
                                 [--write-script-path run-script.sh]

        Options:
            -g / --gds-path: Path to gds directory
            -d / --download-path: The path you'd like to download the data to. Working dir by default.
            -o / --write-script-path: Don't execute the command, just write to a script instead

        Requirements:
          * aws
          * jq     (v1.5+)
          * python3 (v3.4+)

        Environment:
          * ICA_BASE_URL
          * ICA_ACCESS_TOKEN

        Extras:
        *  You can also use any of the aws s3 sync parameters to add to the command list, for example
           gds-sync-download --gds-path gds://volume-name/path-to-folder/ --exclude='*' --include='*.fastq.gz'
           will download only fastq files from that folder.

        If you are unsure on what files will be downloaded, use the --dryrun parameter. This will inform you of which
        files will be downloaded to your local file system.

        Unlike rsync, trailing slashes on the --gds-path and --download-path do not matter. One can assume that
        a trailing slash exists on both parameters. This means that the contents inside the --gds-path parameter are
        downloaded to the contents inside --download-path
        "
}

## Internal functions
echo_stderr(){
  echo "$@" 1>&2
}

binaries_check(){
  : '
  Check each of the required binaries are available
  '
  if ! (type aws jq python3 1>/dev/null); then
    return 1
  fi
}


# Start main

# Set local vars
aws_s3_sync_args=()
gds_path=""
download_path="$PWD"
base_url="${ICA_BASE_URL-}"
access_token="${ICA_ACCESS_TOKEN-}"

# Get args from command line
while [ $# -gt 0 ]; do
  case "$1" in
    -g | --gds-path)
      gds_path="$2"
      shift 1
      ;;
    -d | --download-path)
      download_path="$2"
      shift 1
      ;;
    -o | --write-script-path)
      write_script_path="$2"
      shift 1
      ;;
    -h | --help)
      print_help
      exit 0
      ;;
    --*)
      # Let's add in the parameter arg
      aws_s3_sync_args=("${aws_s3_sync_args[@]}" "$1")
      # First check if $2 is of any length
      if [[ -n "$2" ]]; then
        # Check if the parameter takes a value
        case "$2" in
          --*)
            # Check if just another parameter, ignore for now
            :
            ;;
          *)
            aws_s3_sync_args=("${aws_s3_sync_args[@]}" "$2")
            shift 1
            ;;
        esac
      fi
      ;;
  esac
  shift 1
done

###########ICA-ICA-LAZY SETUP ########################################

if [[ -z "${ICA_ICA_LAZY_HOME-}" ]]; then
  echo "Error - please ensure env var 'ICA_ICA_LAZY_HOME' is set" 1>&2
  exit 1
fi

if [[ -d "${ICA_ICA_LAZY_HOME}/internal-functions" ]]; then
  for f in "${ICA_ICA_LAZY_HOME}/internal-functions/"*".sh"; do
      # shellcheck source=../internal-functions/*.sh
      source "$f"
  done
fi

##########END ICA-ICA-LAZY SETUP ####################################


# Check available binaries exist
if ! binaries_check; then
  echo_stderr "Please make sure binaries aws, jq and python3 are all available on your PATH variable"
  print_help
  exit 1
fi

# Strip base url
base_url="$(strip_path_from_base_url "${base_url}")"

# Check mandatory args are defined
if [[ -z "${gds_path}" ]]; then
  echo_stderr "Please set --gds-path"
  print_help
  exit 1
elif [[ -z "${access_token}" ]]; then
  echo_stderr "Please set the env var ICA_ACCESS_TOKEN"
  echo_stderr "Please first run ica-context-switcher"
  print_help
  exit 1
elif [[ -z "${base_url}" ]]; then
  echo_stderr "Please set the env var ICA_BASE_URL"
  print_help
  exit 1
fi

# Check arguments have a logical value
# We might have set a boolean parameter over a kwarg parameter
if [[ "${gds_path}" =~ ^--.* ]]; then
  echo_stderr "Incorrect usage"
  print_help
  exit 1
fi

if [[ "${download_path}" =~ ^--.* ]]; then
  echo_stderr "Incorrect usage"
  print_help
  exit 1
fi

if [[ "${write_script_path}" =~ ^--.* ]]; then
  echo_stderr "Incorrect usage"
  print_help
  exit 1
fi

# Check token expiry
check_token_expiry "${access_token}"

check_download_path "${download_path}"

# Now run the aws s3 sync command through eval to quote the necessary arguments
# Split volume and path
gds_volume="$(get_volume_from_gds_path "${gds_path}")"
gds_folder_path="$(get_folder_path_from_gds_path "${gds_path}")"

# Get the folder id
folder_id="$(get_folder_id "${gds_volume}" "${gds_folder_path}" "${base_url}" "${access_token}")"

# Check folder id is found
if [[ -z "${folder_id}" || "${folder_id}" == "null" ]]; then
  echo_stderr "Could not get folder id for \"${gds_path}\""
  exit 1
fi

# Get the json aws creds with the curl PATCH command
aws_credentials="$(get_aws_access_creds_from_folder_id "${folder_id}" "${base_url}" "${access_token}")"

# Creds to be exported
aws_access_key_id="$(get_access_key_id_from_credentials "${aws_credentials}")"
aws_secret_access_key="$(get_secret_access_key_from_credentials "${aws_credentials}")"
aws_session_token="$(get_session_token_from_credentials "${aws_credentials}")"
aws_default_region="$(get_region_from_credentials "${aws_credentials}")"

# Components of positional parameter 1
aws_bucket_name="$(get_bucket_name_from_credentials "${aws_credentials}")"
aws_key_prefix="$(get_key_prefix_from_credentials "${aws_credentials}")"

# Check at least one of the important ones is defined
if [[ -z "${aws_access_key_id}" || "${aws_access_key_id}" == "null" ]]; then
  echo_stderr "Could not get aws access key id, are you sure you have write permissions to the folder \"${gds_path}\"?"
  exit 1
fi

# Run command through eval and set env vars
# Export env vars in subshell
export AWS_ACCESS_KEY_ID="${aws_access_key_id}"
export AWS_SECRET_ACCESS_KEY="${aws_secret_access_key}"
export AWS_SESSION_TOKEN="${aws_session_token}"
export AWS_DEFAULT_REGION="${aws_default_region}"

# Don't actually run the command
if [[ -n "${write_script_path}" ]]; then
  echo_stderr "Writing command to ${write_script_path}"
  {
    echo "#!/usr/bin/env bash"
    echo ""
    echo "# Set env vars"
    echo "export AWS_ACCESS_KEY_ID=\"${aws_access_key_id}\""
    echo "export AWS_SECRET_ACCESS_KEY=\"${aws_secret_access_key}\""
    echo "export AWS_SESSION_TOKEN=\"${aws_session_token}\""
    echo "export AWS_DEFAULT_REGION=\"${aws_default_region}\""
    echo ""
    echo "# Run aws s3 sync upload command"
    echo aws s3 sync "s3://${aws_bucket_name}/${aws_key_prefix}" "${download_path}" "${aws_s3_sync_args[@]}"
  } > "${write_script_path}"
  exit 0
fi

# Use trap to catch the command to debug
trap 'previous_command=$this_command; this_command=$BASH_COMMAND' DEBUG
# Launch through eval to expand sync arguments
if eval aws s3 sync "s3://${aws_bucket_name}/${aws_key_prefix}" "${download_path}" '"${aws_s3_sync_args[@]}"'; then
  echo_stderr "Download complete!"
else
  # Get command and return code of aws s3 sync
  s3_command="${previous_command}" non_zero_exit_code="$?"
  # Get S3 command as an array
  IFS=' ' read -r -a s3_command_array <<< "${s3_command}"
  # Drop aws_s3_sync_args array, added at the end
  unset s3_command_array["-1"]
  # Show user previous command
  echo_stderr "Error: Sync command failed with exit code ${non_zero_exit_code}. Sync command was '${s3_command_array[*]} ${aws_s3_sync_args[*]}'"
  # Exit with same failure
  exit "${non_zero_exit_code}"
fi