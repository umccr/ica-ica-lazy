#!/usr/bin/env bash

: '
Migrate from GDS to a AWS S3 bucket using aws s3 sync in a TES task

User must have read access to the source directory, and write access to the destination project

Step 1: Get the urls from the destination project (similar to gds-create-download-script)
Step 2: Create the manifest file and upload to the development project (outside the destination directory)
Step 3: Create output folder in the source directory and get credentials
Step 4: Run TES task - populating the following attributes from the task run template
__GDS_MANIFEST_JSON_URL__
__AWS_S3_BUCKET__
__AWS_S3_PATH__
__AWS_ACCESS_KEY_ID__
__AWS_SECRET_ACCESS_KEY__
__AWS_SESSION_TOKEN__
__GDS_SYSTEM_FILES_PATH__
'

# Set to fail
set -euo pipefail

# Globals
TEMPLATE_NAME="gds-migrate-to-aws-task-run.json"

###########
# FUNCTIONS
###########

# Help function
print_help(){
  echo "
        Usage: gds-migrate-to-aws (--gds-path gds://volume-name/path-to-src-folder/)
                                  (--s3-path s3://bucket-name/path-to-dest-folder/)
                                  (--log-path gds://volume-name/path-to-logs-folder/)
                                  [...additional_rclone_args]

        Description:
          Sync data from gds to AWS using rclone

        Options:
            --gds-path:          Path to gds source directory
            --s3-path:           Path to s3 destination directory
            ...                  Additional arguments are parsed to rclone

        Requirements:
          * aws
          * aws-sso-creds
          * jq     (v1.5+)
          * python3 (v3.4+)

        Environment:
          * ICA_BASE_URL
          * ICA_ACCESS_TOKEN
          * AWS_PROFILE
          * AWS_REGION

        Extras:
            *  You can also use any of the aws s3 sync parameters to add to the command list, for example
               gds-migrate-to-aws --gds-path gds://volume-name/path-to-folder/ --s3-path s3://bucket-name/path-to-folder/ --exclude='*' --include='*.fastq.gz'
               will download only fastq files from that gds folder.

               Unlike rsync, trailing slashes on the --gds-path and --s3-path do not matter. One can assume that
               a trailing slash exists on both parameters. This means that the contents inside the --gds-path parameter are
               downloaded to the contents inside --s3-path

               You should use the --stream option if the output will be relatively small compared to the input.

               aws-sso-creds can be downloaded from the releases page at https://github.com/jaxxstorm/aws-sso-creds

        A token with at least read-only scope must be registered for the source path.

        Example:
          gds-migrate-to-aws --gds-path gds://volume-name/path-to-folder/ --s3-path s3://temp-bucket/folder/ --exclude='*' --include='*.fastq.gz'
        "
}

###########
# GET ARGS
###########

# Get args
src_gds_path=""
src_project=""
dest_s3_path=""
log_path=""
rclone_copy_args_array=""
template_path="${ICA_ICA_LAZY_HOME-}/templates/${TEMPLATE_NAME}"

# Get args from command line
while [ $# -gt 0 ]; do
  case "$1" in
    --gds-path)
      src_gds_path="$2"
      shift 1
      ;;
    --s3-path)
      dest_s3_path="$2"
      shift 1
      ;;
    --log-path)
      log_path="$2"
      shift 1
      ;;
    -h | --help)
      print_help
      exit 0
      ;;
    --*)
      # Let's add in the parameter arg
      rclone_copy_args_array=("${rclone_copy_args_array[@]}" "$1")
      # First check if $2 is of any length
      if [[ -n "${2-}" ]]; then
        # Check if the parameter takes a value
        case "$2" in
          --*)
            # Check if just another parameter, ignore for now
            :
            ;;
          *)
            rclone_copy_args_array=("${rclone_copy_args_array[@]}" "$2")
            shift 1
            ;;
        esac
      fi
      ;;
  esac
  shift 1
done

###########ICA-ICA-LAZY SETUP ########################################

if [[ -z "${ICA_ICA_LAZY_HOME-}" ]]; then
  echo "Error - please ensure env var 'ICA_ICA_LAZY_HOME' is set" 1>&2
  exit 1
fi

if [[ -d "${ICA_ICA_LAZY_HOME}/internal-functions" ]]; then
  for f in "${ICA_ICA_LAZY_HOME}/internal-functions/"*".sh"; do
      # shellcheck source=../internal-functions/*.sh
      source "$f"
  done
fi

##########END ICA-ICA-LAZY SETUP ####################################

# Ensure gds_path params are set
if [[ -z "${src_gds_path}" ]]; then
  echo_stderr "Please make sure --gds-path parameter is set"
  exit 1
fi
if [[ -z "${dest_s3_path}" ]]; then
  echo_stderr "Please make sure --s3-path parameter is set"
  exit 1
fi
if [[ -z "${log_path}" ]]; then
  echo_stderr "Please make sure --log-path parameter is set"
  exit 1
fi


# Start
if [[ -z "${ICA_BASE_URL-}" ]]; then
    echo "Error: Need to set var \"ICA_BASE_URL\"" 1>&2
    exit 1
fi

# Check token
if [[ -z "${ICA_ACCESS_TOKEN}" ]]; then
  echo "Error: ICA ACCESS TOKEN must be set" 1>&2
fi

# Get the src stuff volume name / folder path
src_volume_name="$(get_volume_from_gds_path "${src_gds_path}")"
src_folder_path="$(get_folder_path_from_gds_path "${src_gds_path}")"

if ! check_path_is_folder "${src_volume_name}" "${src_folder_path}" "${ICA_BASE_URL}" "${ICA_ACCESS_TOKEN}"; then
  echo_stderr "Could not confirm ${src_gds_path} was a valid gds path in the ${src_project} context"
  exit 1
fi

# Get the folder in gds
src_folder_id="$(get_folder_id "${src_volume_name}" "${src_folder_path}" "${ICA_BASE_URL}" "${ICA_ACCESS_TOKEN}")"

# Get access credentials from the source project
src_aws_credentials="$(get_aws_access_creds_from_folder_id "${src_folder_id}" "${ICA_BASE_URL}" "${ICA_ACCESS_TOKEN}")"

# Creds to be exported
src_aws_access_key_id="$(get_access_key_id_from_credentials "${src_aws_credentials}")"
src_aws_secret_access_key="$(get_secret_access_key_from_credentials "${src_aws_credentials}")"
src_aws_session_token="$(get_session_token_from_credentials "${src_aws_credentials}")"
src_aws_region="$(get_region_from_credentials "${src_aws_credentials}")"

# Components of positional parameter 1
src_aws_bucket_name="$(get_bucket_name_from_credentials "${src_aws_credentials}")"
src_aws_key_prefix="$(get_key_prefix_from_credentials "${src_aws_credentials}")"

# Get the dest stuff
if [[ -z "${AWS_REGION-}" || "${AWS_DEFAULT_REGION-}" ]]; then
  echo_stderr "Error: AWS_REGION or AWS_DEFAULT_REGION must be set"
  exit 1
fi
if [[ -n "${AWS_REGION-}" ]]; then
  dest_aws_region="${AWS_REGION}"
else
  dest_aws_region="${AWS_DEFAULT_REGION}"
fi

# These commands don't require 'gds://' at the front so usable for s3 too
dest_aws_bucket_name="$(get_volume_from_gds_path "${dest_s3_path}")"
dest_aws_key_prefix="$(get_folder_path_from_gds_path "${dest_s3_path}")"

if [[ -n "${AWS_ACCESS_KEY_ID-}" && -n "${AWS_SECRET_ACCESS_KEY}" && -n "${AWS_SESSION_TOKEN}" ]]; then
  : '
  Were all good here!
  '
  dest_aws_access_key_id="${AWS_ACCESS_KEY_ID}"
  dest_aws_secret_access_key="${AWS_SECRET_ACCESS_KEY}"
  dest_aws_session_token="${AWS_SESSION_TOKEN}"
elif type aws-sso-creds 1>/dev/null 2>&1; then
  if [[ -z "${AWS_PROFILE-}" && -z "${AWS_DEFAULT_PROFILE-}" ]]; then
    echo_stderr "Cannot use aws-sso-creds without knowing the aws profile. Please set either AWS_PROFILE or AWS_DEFAULT_PROFILE environment vars and try again"
  fi
  # Crete aws access key id
  aws_sso_creds_str="$( \
    aws-sso-creds get | \
    "$(get_sed_binary)" -E 's/\s+/ /' \
  )"

  dest_aws_access_key_id="$(
    grep "AWS_ACCESS_KEY_ID" <<< "${aws_sso_creds_str}" | cut -d' ' -f2
  )"

  dest_aws_secret_access_key="$(
    grep "AWS_SECRET_ACCESS_KEY" <<< "${aws_sso_creds_str}" | cut -d' ' -f2
  )"

  dest_aws_session_token="$(
    grep "AWS_SESSION_TOKEN" <<< "${aws_sso_creds_str}" | cut -d' ' -f2
  )"
else
  echo_stderr "No way to get AWS credentials, please install aws-sso-creds or set the following environment vars"
  echo_stderr "AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN"
fi

# Get the log stuff
log_volume_name="$(get_volume_from_gds_path "${log_path}")"
log_folder_path="$(get_folder_path_from_gds_path "${log_path}")"

# Create temp file with template
temp_tes_migration_path="$("$(get_mktemp_binary)" -t "tes-migrate.XXX.json")"
cp "${template_path}" "${temp_tes_migration_path}"

# Update template TES json
echo_stderr "Populating TES task template"
"$(get_sed_binary)" -i "s%__SRC_AWS_REGION__%${src_aws_region}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__SRC_AWS_ACCESS_KEY_ID__%${src_aws_access_key_id}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__SRC_AWS_SECRET_ACCESS_KEY__%${src_aws_secret_access_key}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__SRC_AWS_SESSION_TOKEN__%${src_aws_session_token}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__SRC_AWS_BUCKET_NAME__%${src_aws_bucket_name}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__SRC_AWS_KEY_PREFIX__%${src_aws_key_prefix}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__DEST_AWS_REGION__%${dest_aws_region}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__DEST_AWS_ACCESS_KEY_ID__%${dest_aws_access_key_id}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__DEST_AWS_SECRET_ACCESS_KEY__%${dest_aws_secret_access_key}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__DEST_AWS_SESSION_TOKEN__%${dest_aws_session_token}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__DEST_AWS_BUCKET_NAME__%${dest_aws_bucket_name}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__DEST_AWS_KEY_PREFIX__%${dest_aws_key_prefix}%" "${temp_tes_migration_path}"
"$(get_sed_binary)" -i "s%__GDS_SYSTEM_FILES_PATH__%gds://${log_volume_name}${log_folder_path%/}%" "${temp_tes_migration_path}"

# Add in rclone args
for rclone_copy_arg in "${rclone_copy_args_array[@]}"; do
  if [[ -n "${rclone_copy_arg}" ]]; then
    rclone_copy_str+=" \\\\"'"'"${rclone_copy_arg}\\\\"'"'
  fi
done

if [[ -v rclone_copy_str ]]; then
  "$(get_sed_binary)" -i "s%__ADDITIONAL_RCLONE_SYNC_ARGS__%${rclone_copy_str}%" "${temp_tes_migration_path}"
else
  "$(get_sed_binary)" -i "s%__ADDITIONAL_RCLONE_SYNC_ARGS__%%" "${temp_tes_migration_path}"
fi


# Launch the tes task
echo_stderr "Launching TES task"
tes_task_id="$( \
  curl \
    --silent \
    --fail-with-body \
    --show-error \
    --location \
    --request POST \
    --header 'Accept: application/json' \
    --header 'Content-Type: application/*+json' \
    --header "Authorization: Bearer ${ICA_ACCESS_TOKEN}" \
    --data "@${temp_tes_migration_path}" \
    --url "${ICA_BASE_URL}/v1/tasks/runs" |
    jq --raw-output \
      '.id' \
)"

echo_stderr "Launching data transfer with task run ${tes_task_id}"

rm "${temp_tes_migration_path}"